Katelyn
I initally worked on implementing word embeddings to improve sentence selection. I got the word embedding for the sentence of the question using sen2vec from the word2vec_extractor file and then I got the word embedding for each possible sentence. From there, I took the cosine similarity to find which sentence would best match with the question. This resulted in a 75% accuracy in Recall compared to 83% from our last assignment. In this case, we decided to move on from word embeddings and keep our original sentence selection and experiment more with word embeddings in the next phase.

I then worked on phrase selection with the "why" questions. I looked for the word "because" in the list of candidate sentences and grab the rest of the sentence after that. If the sentence does not have the word "because", I grab the last word of the question sentence, and search for that word in list of the candidate sentences. If that word is found, I return everything after that word because I saw this was a typical pattern from looking at the sentences on the Stanford parser. If the sentence I'm looking for does not have the word, because it is the wrong sentence, then all the possible sentences are returned. This method has returned majority of why questions except for gold answers that have inconsistencies.

